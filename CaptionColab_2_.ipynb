{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kush-Singh-26/Image_Caption/blob/main/CaptionColab_2_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Continued"
      ],
      "metadata": {
        "id": "DjC_oRDNpcQN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07hOzTYzu2Xb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "import nltk\n",
        "import json\n",
        "import collections\n",
        "from collections import Counter\n",
        "import random\n",
        "import time\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/coco/images/train2014\n",
        "!mkdir -p /content/coco/images/val2014\n",
        "!mkdir -p /content/coco/annotations\n"
      ],
      "metadata": {
        "id": "eGjHBDsJu7TL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://images.cocodataset.org/zips/train2014.zip -P /content/coco/images/\n",
        "!wget http://images.cocodataset.org/zips/val2014.zip -P /content/coco/images/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtqN71CEu9lx",
        "outputId": "770b2d3a-3586-4e19-c898-a7960e373862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-05 03:53:21--  http://images.cocodataset.org/zips/train2014.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.192.209, 52.217.118.89, 52.216.59.185, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.192.209|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13510573713 (13G) [application/zip]\n",
            "Saving to: ‘/content/coco/images/train2014.zip’\n",
            "\n",
            "train2014.zip       100%[===================>]  12.58G  50.9MB/s    in 4m 21s  \n",
            "\n",
            "2025-05-05 03:57:42 (49.3 MB/s) - ‘/content/coco/images/train2014.zip’ saved [13510573713/13510573713]\n",
            "\n",
            "--2025-05-05 03:57:42--  http://images.cocodataset.org/zips/val2014.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 16.15.177.212, 16.15.194.211, 52.217.229.249, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|16.15.177.212|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6645013297 (6.2G) [application/zip]\n",
            "Saving to: ‘/content/coco/images/val2014.zip’\n",
            "\n",
            "val2014.zip         100%[===================>]   6.19G  44.8MB/s    in 2m 2s   \n",
            "\n",
            "2025-05-05 03:59:45 (51.8 MB/s) - ‘/content/coco/images/val2014.zip’ saved [6645013297/6645013297]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip -P /content/coco/annotations/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24nq-sl1u_Hb",
        "outputId": "77625bf2-05cf-4fb5-e280-3bd0eb9bf030"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-05 03:59:45--  http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.124.233, 16.15.184.101, 52.217.85.132, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.124.233|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252872794 (241M) [application/zip]\n",
            "Saving to: ‘/content/coco/annotations/annotations_trainval2014.zip’\n",
            "\n",
            "annotations_trainva 100%[===================>] 241.16M  58.1MB/s    in 4.6s    \n",
            "\n",
            "2025-05-05 03:59:50 (51.9 MB/s) - ‘/content/coco/annotations/annotations_trainval2014.zip’ saved [252872794/252872794]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/coco/images/train2014.zip -d /content/coco/images/\n",
        "!unzip -q /content/coco/images/val2014.zip -d /content/coco/images/\n",
        "!unzip -q /content/coco/annotations/annotations_trainval2014.zip -d /content/coco/annotations/\n"
      ],
      "metadata": {
        "id": "n_pnFNrCvBPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -c \"https://github.com/Delphboy/karpathy-splits/raw/main/dataset_coco.json?download=\" -O /content/dataset_coco.json\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xr0hvNAuvDL6",
        "outputId": "d601adff-2452-46fe-ca4c-82f6993949fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-05 04:03:44--  https://github.com/Delphboy/karpathy-splits/raw/main/dataset_coco.json?download=\n",
            "Resolving github.com (github.com)... 140.82.116.3\n",
            "Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://media.githubusercontent.com/media/Delphboy/karpathy-splits/main/dataset_coco.json?download=true [following]\n",
            "--2025-05-05 04:03:44--  https://media.githubusercontent.com/media/Delphboy/karpathy-splits/main/dataset_coco.json?download=true\n",
            "Resolving media.githubusercontent.com (media.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to media.githubusercontent.com (media.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 408860810 (390M) [application/octet-stream]\n",
            "Saving to: ‘/content/dataset_coco.json’\n",
            "\n",
            "/content/dataset_co 100%[===================>] 389.92M  71.5MB/s    in 3.8s    \n",
            "\n",
            "2025-05-05 04:03:55 (102 MB/s) - ‘/content/dataset_coco.json’ saved [408860810/408860810]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"/content/dataset_coco.json\", \"r\") as f:\n",
        "    karpathy_data = json.load(f)\n",
        "\n",
        "karpathy_images = karpathy_data['images']\n",
        "\n",
        "train_data = [img for img in karpathy_images if img['split'] == 'train']\n",
        "val_data = [img for img in karpathy_images if img['split'] == 'val']\n",
        "test_data = [img for img in karpathy_images if img['split'] == 'test']\n",
        "\n",
        "print(f\"# Train: {len(train_data)} | Val: {len(val_data)} | Test: {len(test_data)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfUIz7XmvFVh",
        "outputId": "4f7e78e1-4e3a-4c7a-b6df-b5e31bccc442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Train: 82783 | Val: 5000 | Test: 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "from collections import Counter\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6X1gX-evHP0",
        "outputId": "bcd17acb-e4e0-4603-efc3-2f2f6ded0535"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary:\n",
        "    def __init__(self, freq_threshold=5):\n",
        "        self.freq_threshold = freq_threshold\n",
        "        # Initialize with special tokens\n",
        "        self.word2idx = {\"<pad>\": 0, \"<start>\": 1, \"<end>\": 2, \"<unk>\": 3}\n",
        "        self.idx2word = {0: \"<pad>\", 1: \"<start>\", 2: \"<end>\", 3: \"<unk>\"}\n",
        "        self.idx = 4 # Next index to assign\n",
        "\n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        frequencies = Counter()\n",
        "        print(f\"Building vocabulary from {len(sentence_list)} sentences...\")\n",
        "        for i, sentence in enumerate(sentence_list):\n",
        "            tokens = nltk.tokenize.word_tokenize(sentence.lower())\n",
        "            frequencies.update(tokens)\n",
        "            if (i+1) % 100000 == 0:\n",
        "                 print(f\"Processed {i+1}/{len(sentence_list)} sentences\")\n",
        "\n",
        "\n",
        "        original_size = len(frequencies)\n",
        "        filtered_freq = {word: freq for word, freq in frequencies.items() if freq >= self.freq_threshold}\n",
        "\n",
        "        for word in filtered_freq:\n",
        "            if word not in self.word2idx:\n",
        "                self.word2idx[word] = self.idx\n",
        "                self.idx2word[self.idx] = word\n",
        "                self.idx += 1\n",
        "        print(f\"Original vocab size: {original_size}, Filtered size (freq>={self.freq_threshold}): {len(self.word2idx)}\")\n",
        "\n",
        "\n",
        "    def numericalize(self, text):\n",
        "        tokens = nltk.tokenize.word_tokenize(text.lower())\n",
        "        return [self.word2idx.get(token, self.word2idx[\"<unk>\"]) for token in tokens]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.idx"
      ],
      "metadata": {
        "id": "O-Dx-VyUmQvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self, freq_threshold=5):\n",
        "        self.freq_threshold = freq_threshold\n",
        "        self.word2idx = {\"<pad>\": 0, \"<start>\": 1, \"<end>\": 2, \"<unk>\": 3}\n",
        "        self.idx2word = {0: \"<pad>\", 1: \"<start>\", 2: \"<end>\", 3: \"<unk>\"}\n",
        "        self.idx = 4\n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        frequencies = Counter()\n",
        "        for sentence in sentence_list:\n",
        "            tokens = nltk.tokenize.word_tokenize(sentence.lower())\n",
        "            frequencies.update(tokens)\n",
        "        filtered_freq = {word: freq for word, freq in frequencies.items() if freq >= self.freq_threshold}\n",
        "        for word in filtered_freq:\n",
        "            if word not in self.word2idx:\n",
        "                self.word2idx[word] = self.idx\n",
        "                self.idx2word[self.idx] = word\n",
        "                self.idx += 1\n",
        "    def numericalize(self, text):\n",
        "        tokens = nltk.tokenize.word_tokenize(text.lower())\n",
        "        return [self.word2idx.get(token, self.word2idx[\"<unk>\"]) for token in tokens]\n",
        "    def __len__(self):\n",
        "        return self.idx\n",
        "\n",
        "vocab_pickle_path = '/content/vocab.pkl'\n",
        "\n",
        "\n",
        "vocab = None\n",
        "print(f\"\\n--- Attempting to load Vocabulary from Pickle: {vocab_pickle_path} ---\")\n",
        "\n",
        "if not os.path.exists(vocab_pickle_path):\n",
        "    print(f\"Error: Vocabulary pickle file not found at {vocab_pickle_path}\")\n",
        "    exit()\n",
        "else:\n",
        "    try:\n",
        "        # Open the file in 'rb' (read binary) mode\n",
        "        with open(vocab_pickle_path, 'rb') as f:\n",
        "            # Load the object from the file\n",
        "            vocab = pickle.load(f)\n",
        "\n",
        "        if isinstance(vocab, Vocabulary):\n",
        "            vocab_size = len(vocab)\n",
        "            print(f\"Vocabulary loaded successfully. Size: {vocab_size}\")\n",
        "\n",
        "        else:\n",
        "            print(f\"Error: Loaded object is not an instance of the Vocabulary class!\")\n",
        "            vocab = None # Reset vocab if loading failed verification\n",
        "            exit() # Exit if the correct object wasn't loaded\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Vocabulary pickle file not found at {vocab_pickle_path}\")\n",
        "        exit()\n",
        "    except pickle.UnpicklingError:\n",
        "        print(f\"Error: Could not unpickle the file. It might be corrupted or saved with a different protocol.\")\n",
        "        exit()\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred while loading the vocabulary pickle file: {e}\")\n",
        "        exit()\n",
        "\n",
        "if vocab:\n",
        "    print(\"\\nVocabulary object is ready to use.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nFailed to load or initialize vocabulary. Cannot proceed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEybY7J7mJcT",
        "outputId": "fd407372-12b0-4c93-d946-2d097a2b8bf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Attempting to load Vocabulary from Pickle: /content/vocab.pkl ---\n",
            "Vocabulary loaded successfully. Size: 8853\n",
            "\n",
            "Vocabulary object is ready to use.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderCNN(nn.Module):\n",
        "  def __init__(self, embed_size, dropout_p=0.5, fine_tune=True): # Added fine_tune flag\n",
        "    super(EncoderCNN, self).__init__()\n",
        "    print(f\"Initializing EncoderCNN: embed_size={embed_size}, dropout={dropout_p}, fine_tune={fine_tune}\")\n",
        "    resnet = models.resnet101(weights=models.ResNet101_Weights.IMAGENET1K_V1) # Use updated weights call\n",
        "\n",
        "    # Freeze all layers initially\n",
        "    for param in resnet.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "    if fine_tune:\n",
        "      print(\"Fine-tuning ResNet: Unfreezing layer4 parameters.\")\n",
        "      for param in resnet.layer4.parameters(): # Unfreeze layer 4\n",
        "          param.requires_grad = True\n",
        "\n",
        "    self.resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
        "\n",
        "    # Add trainable layers\n",
        "    self.fc = nn.Linear(resnet.fc.in_features, embed_size)\n",
        "    self.bn = nn.BatchNorm1d(embed_size, momentum=0.01) # BatchNorm after FC\n",
        "    self.dropout = nn.Dropout(dropout_p) # Dropout layer\n",
        "\n",
        "    # Initialize weights for the new layers\n",
        "    self.fc.weight.data.normal_(0.0, 0.02)\n",
        "    self.fc.bias.data.fill_(0)\n",
        "\n",
        "  def forward(self, images):\n",
        "    with torch.no_grad() if not self.training else torch.enable_grad():\n",
        "        features = self.resnet(images) # [B, C, 1, 1]\n",
        "\n",
        "    features = features.squeeze(3).squeeze(2) # [B, C]\n",
        "    features = self.fc(features)              # [B, E]\n",
        "    features = self.bn(features)              # [B, E] - Apply BN before dropout\n",
        "    features = self.dropout(features)         # [B, E] - Apply dropout\n",
        "    return features\n"
      ],
      "metadata": {
        "id": "qtgKy2UQvjD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1, dropout_p=0.5):\n",
        "        super().__init__()\n",
        "        print(f\"Initializing DecoderRNN: embed_size={embed_size}, hidden_size={hidden_size}, vocab_size={vocab_size}, num_layers={num_layers}, dropout={dropout_p}\")\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.embed_dropout = nn.Dropout(dropout_p) # Dropout after embedding\n",
        "        # Apply LSTM dropout between layers only if num_layers > 1\n",
        "        lstm_dropout = dropout_p if num_layers > 1 else 0\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=lstm_dropout)\n",
        "        self.dropout = nn.Dropout(dropout_p) # Dropout before final linear layer\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        # Layers to initialize LSTM state from image features\n",
        "        self.init_h = nn.Linear(embed_size, hidden_size)\n",
        "        self.init_c = nn.Linear(embed_size, hidden_size)\n",
        "\n",
        "        # Initialize weights\n",
        "        self.embed.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.linear.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.linear.bias.data.fill_(0)\n",
        "        self.init_h.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.init_h.bias.data.fill_(0)\n",
        "        self.init_c.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.init_c.bias.data.fill_(0)\n",
        "\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        # features: [B, E], captions: [B, T] (T = sequence length)\n",
        "\n",
        "        # Prepare initial LSTM state from image features\n",
        "        # Need shape [num_layers, B, H]\n",
        "        h0 = self.init_h(features).unsqueeze(0) # [1, B, H]\n",
        "        c0 = self.init_c(features).unsqueeze(0) # [1, B, H]\n",
        "        # If num_layers > 1, repeat the initial state for each layer\n",
        "        if self.lstm.num_layers > 1:\n",
        "             h0 = h0.repeat(self.lstm.num_layers, 1, 1)\n",
        "             c0 = c0.repeat(self.lstm.num_layers, 1, 1)\n",
        "\n",
        "        # Embed captions and apply dropout\n",
        "        embeddings = self.embed(captions)    # [B, T, E]\n",
        "        embeddings = self.embed_dropout(embeddings) # Apply dropout\n",
        "\n",
        "        # Pass through LSTM\n",
        "        # embeddings shape needs to be [B, T, E] for batch_first=True\n",
        "        hiddens, _ = self.lstm(embeddings, (h0, c0))  # [B, T, H]\n",
        "\n",
        "        # Apply dropout before the final linear layer\n",
        "        hiddens = self.dropout(hiddens) # Apply dropout\n",
        "\n",
        "        # Generate outputs (logits over vocabulary)\n",
        "        outputs = self.linear(hiddens)  # [B, T, Vocab]\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "s1STLQa5v5jP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CocoDataset(Dataset):\n",
        "    def __init__(self, data, img_root, vocab, transform=None):\n",
        "        self.data = data\n",
        "        self.img_root = img_root\n",
        "        self.vocab = vocab\n",
        "        self.transform = transform\n",
        "        print(f\"Initialized CocoDataset with {len(self.data)} items. Root: {self.img_root}\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        entry = self.data[idx]\n",
        "\n",
        "        # Randomly select one caption\n",
        "        caption_entry = random.choice(entry['sentences'])\n",
        "        caption = caption_entry['raw']\n",
        "\n",
        "        # Numericalize caption\n",
        "        tokens = [self.vocab.word2idx[\"<start>\"]] + \\\n",
        "                 self.vocab.numericalize(caption) + \\\n",
        "                 [self.vocab.word2idx[\"<end>\"]]\n",
        "\n",
        "        # Load image\n",
        "        split_folder = entry.get('filepath', '')\n",
        "\n",
        "        if split_folder in self.img_root:\n",
        "             img_path = os.path.join(self.img_root, entry['filename'])\n",
        "        else:\n",
        "             img_path = os.path.join(self.img_root, split_folder, entry['filename'])\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "        except FileNotFoundError:\n",
        "             print(f\"Warning: Image not found at {img_path}. Skipping.\")\n",
        "\n",
        "             return self.__getitem__(0)\n",
        "\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, torch.Tensor(tokens).long() # Ensure tokens are LongTensor\n"
      ],
      "metadata": {
        "id": "2_uerJvcv9Bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "\n",
        "# Training Transform (with augmentation)\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)), # Random crop and resize\n",
        "    transforms.RandomHorizontalFlip(),      # Random horizontal flip\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), # Color augmentation\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), # Resize to fixed size\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])"
      ],
      "metadata": {
        "id": "xGJ7Os3qwM_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    # Separate images and captions\n",
        "    images, captions = zip(*batch)\n",
        "\n",
        "    # Stack images (they should all be the same size)\n",
        "    images = torch.stack(images, 0)\n",
        "\n",
        "    # Pad captions to the max length in the batch\n",
        "    # Make sure captions are Tensors before passing to pad_sequence\n",
        "    captions = pad_sequence(captions, batch_first=True, padding_value=vocab.word2idx[\"<pad>\"])\n",
        "\n",
        "    return images, captions"
      ],
      "metadata": {
        "id": "_7LnxrIAwR-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = '/content/best_model_improved.pth'\n",
        "num_epochs = 15 # Total desired epochs (including previous ones)\n",
        "\n",
        "# Load hyperparameters from checkpoint if available, otherwise use defaults\n",
        "print(f\"--- Loading Checkpoint: {checkpoint_path} ---\")\n",
        "if not os.path.exists(checkpoint_path):\n",
        "    print(f\"Error: Checkpoint file not found at {checkpoint_path}\")\n",
        "    exit()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
        "print(f\"Checkpoint loaded successfully from device {checkpoint.get('device', 'unknown') if isinstance(checkpoint, dict) else 'unknown'}\") # Check device if saved\n",
        "\n",
        "# Extract parameters from checkpoint (provide defaults if keys might be missing)\n",
        "embed_size = checkpoint.get('embed_size', 256)\n",
        "hidden_size = checkpoint.get('hidden_size', 512)\n",
        "num_layers = checkpoint.get('num_layers', 1)\n",
        "dropout_prob = checkpoint.get('dropout_prob', 0.5)\n",
        "fine_tune_encoder = checkpoint.get('fine_tune_encoder', True)\n",
        "learning_rate = 3e-4   # Base LR - Can be loaded from optimizer state later\n",
        "fine_tune_lr = 1e-5    # Fine-tune LR - Can be loaded from optimizer state later\n",
        "weight_decay = 1e-5    # Weight decay\n",
        "patience_early_stop = 3 # Needs to match the EarlyStopping instance setup\n",
        "delta_early_stop = 0.005\n",
        "patience_scheduler = 1\n",
        "\n",
        "batch_size = 64        # Batch size (keep consistent or adjust based on memory)\n",
        "num_workers = 2 if device.type == 'cuda' else 0 # Dataloader workers\n",
        "\n",
        "print(\"\\n--- Using Parameters ---\")\n",
        "print(f\"  Embed Size: {embed_size}\")\n",
        "print(f\"  Hidden Size: {hidden_size}\")\n",
        "print(f\"  Num Layers (LSTM): {num_layers}\")\n",
        "print(f\"  Dropout: {dropout_prob}\")\n",
        "print(f\"  Fine-tune Encoder: {fine_tune_encoder}\")\n",
        "print(f\"  Target Num Epochs: {num_epochs}\")\n",
        "print(f\"  Device: {device}\")\n",
        "\n",
        "\n",
        "# Load Vocabulary\n",
        "print(\"\\n--- Loading Vocabulary ---\")\n",
        "if 'vocab' not in checkpoint:\n",
        "    print(\"Error: Vocabulary not found in checkpoint. Cannot resume precisely.\")\n",
        "    exit()\n",
        "vocab = checkpoint['vocab']\n",
        "vocab_size = len(vocab)\n",
        "print(f\"Vocabulary loaded. Size: {vocab_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPbh5PannSOF",
        "outputId": "a1b38259-542b-4bc0-c9cb-31acc74495db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Loading Checkpoint: /content/best_model_improved.pth ---\n",
            "Checkpoint loaded successfully from device unknown\n",
            "\n",
            "--- Using Parameters ---\n",
            "  Embed Size: 256\n",
            "  Hidden Size: 512\n",
            "  Num Layers (LSTM): 1\n",
            "  Dropout: 0.5\n",
            "  Fine-tune Encoder: True\n",
            "  Target Num Epochs: 15\n",
            "  Device: cuda\n",
            "\n",
            "--- Loading Vocabulary ---\n",
            "Vocabulary loaded. Size: 8853\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- Instantiate Datasets and Dataloaders ---\n",
        "train_img_root = '/content/coco/images/' # Base directory containing train2014, val2014 etc.\n",
        "val_img_root = '/content/coco/images/'\n",
        "\n",
        "train_dataset = CocoDataset(train_data, train_img_root, vocab, train_transform)\n",
        "val_dataset = CocoDataset(val_data, val_img_root, vocab, val_transform)\n",
        "\n",
        "num_workers = 2 if device.type == 'cuda' else 0\n",
        "print(f\"Using {num_workers} workers for DataLoaders.\")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=num_workers, pin_memory=True if device.type == 'cuda' else False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=num_workers, pin_memory=True if device.type == 'cuda' else False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZ9GJ7snwWPM",
        "outputId": "61e49d6a-4a70-47e8-e805-4a4d48405a72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Initialized CocoDataset with 82783 items. Root: /content/coco/images/\n",
            "Initialized CocoDataset with 5000 items. Root: /content/coco/images/\n",
            "Using 2 workers for DataLoaders.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Initializing Models, Optimizer, etc. ---\")\n",
        "encoder = EncoderCNN(embed_size, dropout_p=dropout_prob, fine_tune=fine_tune_encoder).to(device)\n",
        "decoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers, dropout_p=dropout_prob).to(device)\n",
        "\n",
        "decoder_params = list(decoder.parameters())\n",
        "encoder_fc_params = list(encoder.fc.parameters()) + list(encoder.bn.parameters())\n",
        "encoder_finetune_params = []\n",
        "if fine_tune_encoder:\n",
        "    layer4_index = 7\n",
        "    encoder_finetune_params = list(encoder.resnet[layer4_index].parameters())\n",
        "    print(f\"Optimizing {len(encoder_finetune_params)} parameter tensors from ResNet layer4 with LR {fine_tune_lr}\")\n",
        "\n",
        "\n",
        "# Base parameters (Decoder + Encoder FC/BN layers)\n",
        "params_to_optimize = [\n",
        "    {'params': decoder_params},\n",
        "    {'params': encoder_fc_params}\n",
        "]\n",
        "\n",
        "if encoder_finetune_params:\n",
        "    params_to_optimize.append({'params': encoder_finetune_params, 'lr': fine_tune_lr})\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = optim.Adam(params_to_optimize, lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "# Learning Rate Scheduler\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=patience_scheduler, verbose=True)\n",
        "\n",
        "# Loss Function (ignore padding)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=vocab.word2idx[\"<pad>\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhHnJr7FwtTK",
        "outputId": "26c438e0-ce68-4b04-af94-7194fbe8ef79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Initializing Models, Optimizer, etc. ---\n",
            "Initializing EncoderCNN: embed_size=256, dropout=0.5, fine_tune=True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n",
            "100%|██████████| 171M/171M [00:01<00:00, 152MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning ResNet: Unfreezing layer4 parameters.\n",
            "Initializing DecoderRNN: embed_size=256, hidden_size=512, vocab_size=8853, num_layers=1, dropout=0.5\n",
            "Optimizing 30 parameter tensors from ResNet layer4 with LR 1e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3, delta=0.0):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "        self.delta = delta # Minimum change to qualify as an improvement\n",
        "        print(f\"Initialized EarlyStopping: patience={patience}, delta={delta}\")\n",
        "\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "            print(f\"EarlyStopping: Initial best loss set to {val_loss:.4f}\")\n",
        "        # Check if val_loss has improved significantly\n",
        "        elif val_loss < self.best_loss - self.delta:\n",
        "             print(f\"EarlyStopping: Validation loss improved ({self.best_loss:.4f} --> {val_loss:.4f}). Resetting counter.\")\n",
        "             self.best_loss = val_loss\n",
        "             self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            print(f\"EarlyStopping: No significant improvement for {self.counter}/{self.patience} epochs.\")\n",
        "            if self.counter >= self.patience:\n",
        "                print(\"EarlyStopping: Triggering early stop.\")\n",
        "                self.early_stop = True\n",
        "\n",
        "early_stopper = EarlyStopping(patience=patience_early_stop, delta=delta_early_stop)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Mxt3Clew3Yo",
        "outputId": "60516fdc-7b16-450b-ed07-8b40d679383c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized EarlyStopping: patience=3, delta=0.005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Loading State Dictionaries ---\")\n",
        "encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
        "decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "# Load scheduler state if it was saved\n",
        "if 'scheduler_state_dict' in checkpoint:\n",
        "     try:\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        print(\"Scheduler state loaded.\")\n",
        "     except Exception as e:\n",
        "         print(f\"Warning: Could not load scheduler state: {e}\")\n",
        "else:\n",
        "     print(\"Scheduler state not found in checkpoint, using new scheduler state.\")\n",
        "\n",
        "print(\"Model and optimizer states loaded.\")\n",
        "\n",
        "# Restore Training Variables\n",
        "start_epoch = checkpoint.get('epoch', 0) # Get epoch completed\n",
        "# If epoch is the completed one, start training from the next\n",
        "resume_epoch = start_epoch # The loop will run from here\n",
        "\n",
        "best_val_loss = checkpoint.get('val_loss', float('inf')) # Get the best loss achieved\n",
        "\n",
        "# Initialize Early Stopping (match original parameters)\n",
        "early_stopper = EarlyStopping(patience=patience_early_stop, delta=delta_early_stop)\n",
        "# Set the early stopper's best loss to the loaded value\n",
        "early_stopper.best_loss = best_val_loss\n",
        "\n",
        "early_stopper.counter = checkpoint.get('early_stop_counter', 0)\n",
        "early_stopper.early_stop = checkpoint.get('early_stop_flag', False) # Check if it already triggered\n",
        "\n",
        "\n",
        "print(f\"\\n--- Resuming Training from Epoch {resume_epoch + 1} ---\")\n",
        "print(f\"Previous best validation loss: {best_val_loss:.4f}\")\n",
        "if early_stopper.early_stop:\n",
        "    print(\"Early stopping was triggered in the previous run. Exiting.\")\n",
        "    exit()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzrOEB0Iw8KK",
        "outputId": "a433db78-8feb-459d-ddee-fba607a3a48b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Loading State Dictionaries ---\n",
            "Scheduler state not found in checkpoint, using new scheduler state.\n",
            "Model and optimizer states loaded.\n",
            "Initialized EarlyStopping: patience=3, delta=0.005\n",
            "\n",
            "--- Resuming Training from Epoch 11 ---\n",
            "Previous best validation loss: 2.4801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtA7KX3gpeOk",
        "outputId": "db3c8fd7-541b-4e40-9ac9-aea2beb0a4dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Starting Resumed Training Loop ---\")\n",
        "for epoch in range(resume_epoch, num_epochs):\n",
        "    current_epoch_number = epoch + 1\n",
        "    start_time = time.time()\n",
        "    epoch_train_loss = 0.0\n",
        "\n",
        "    # Training Phase\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    print(f\"\\nEpoch [{current_epoch_number}/{num_epochs}] - Training\")\n",
        "    for i, (images, captions) in enumerate(train_loader):\n",
        "        images, captions = images.to(device), captions.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        features = encoder(images)\n",
        "        outputs = decoder(features, captions[:, :-1])\n",
        "        loss = criterion(outputs.reshape(-1, vocab_size), captions[:, 1:].reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_train_loss += loss.item()\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"Batch [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_train_loss = epoch_train_loss / len(train_loader)\n",
        "    epoch_time = time.time() - start_time\n",
        "    print(f\"Epoch [{current_epoch_number}/{num_epochs}] Training completed in {epoch_time:.2f}s\")\n",
        "    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Validation Phase\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    epoch_val_loss = 0.0\n",
        "    print(f\"Epoch [{current_epoch_number}/{num_epochs}] - Validation\")\n",
        "    with torch.no_grad():\n",
        "        for val_images, val_captions in val_loader:\n",
        "            val_images, val_captions = val_images.to(device), val_captions.to(device)\n",
        "            val_features = encoder(val_images)\n",
        "            val_outputs = decoder(val_features, val_captions[:, :-1])\n",
        "            val_loss = criterion(val_outputs.reshape(-1, vocab_size), val_captions[:, 1:].reshape(-1))\n",
        "            epoch_val_loss += val_loss.item()\n",
        "\n",
        "    avg_val_loss = epoch_val_loss / len(val_loader)\n",
        "    print(f\"Epoch [{current_epoch_number}/{num_epochs}], Average Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Checkpoint Saving\n",
        "    # Compare against the potentially updated best_val_loss\n",
        "    if avg_val_loss < best_val_loss - delta_early_stop:\n",
        "        print(f\"Validation loss decreased ({best_val_loss:.4f} --> {avg_val_loss:.4f}). Saving model...\")\n",
        "        best_val_loss = avg_val_loss # Update best loss tracker\n",
        "        save_dict = {\n",
        "            'epoch': current_epoch_number, # Save the epoch number\n",
        "            'encoder_state_dict': encoder.state_dict(),\n",
        "            'decoder_state_dict': decoder.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'vocab': vocab,\n",
        "            'val_loss': best_val_loss,\n",
        "            # Save hyperparameters used for this run\n",
        "            'embed_size': embed_size,\n",
        "            'hidden_size': hidden_size,\n",
        "            'num_layers': num_layers,\n",
        "            'dropout_prob': dropout_prob,\n",
        "            'fine_tune_encoder': fine_tune_encoder,\n",
        "            'device': str(device), # Store device maybe\n",
        "            'early_stop_counter': early_stopper.counter,\n",
        "            'early_stop_flag': early_stopper.early_stop\n",
        "        }\n",
        "        torch.save(save_dict, checkpoint_path) # Overwrite the best model file\n",
        "    else:\n",
        "         print(f\"Validation loss did not improve significantly from {best_val_loss:.4f}.\")\n",
        "\n",
        "    # Learning Rate Scheduling\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "    # Early Stopping Check\n",
        "    # Sync the early stopper's internal best loss before calling it\n",
        "    early_stopper.best_loss = best_val_loss\n",
        "    early_stopper(avg_val_loss)\n",
        "    # Update best_val_loss in case early_stopper internally updates it (though current impl doesn't)\n",
        "    best_val_loss = early_stopper.best_loss\n",
        "\n",
        "    if early_stopper.early_stop:\n",
        "        print(\"Early stopping criteria met. Stopping training.\")\n",
        "        break\n",
        "\n",
        "print(\"\\n--- Resumed Training Finished ---\")\n",
        "print(f\"Best Validation Loss achieved: {best_val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I60IIFFMw__X",
        "outputId": "00e9b112-d1e3-4ee7-cd8e-bef5da42d625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Resumed Training Loop ---\n",
            "\n",
            "Epoch [11/15] - Training\n",
            "Batch [100/1294], Loss: 2.4579\n",
            "Batch [200/1294], Loss: 2.5990\n",
            "Batch [300/1294], Loss: 2.4190\n",
            "Batch [400/1294], Loss: 2.5273\n",
            "Batch [500/1294], Loss: 2.4743\n",
            "Batch [600/1294], Loss: 2.8252\n",
            "Batch [700/1294], Loss: 2.7027\n",
            "Batch [800/1294], Loss: 2.5735\n",
            "Batch [900/1294], Loss: 2.6313\n",
            "Batch [1000/1294], Loss: 2.5925\n",
            "Batch [1100/1294], Loss: 2.4983\n",
            "Batch [1200/1294], Loss: 2.6223\n",
            "Epoch [11/15] Training completed in 1182.73s\n",
            "Average Training Loss: 2.6228\n",
            "Epoch [11/15] - Validation\n",
            "Epoch [11/15], Average Validation Loss: 2.5042\n",
            "Validation loss did not improve significantly from 2.4801.\n",
            "EarlyStopping: No significant improvement for 1/3 epochs.\n",
            "\n",
            "Epoch [12/15] - Training\n",
            "Batch [100/1294], Loss: 2.7597\n",
            "Batch [200/1294], Loss: 2.5869\n",
            "Batch [300/1294], Loss: 2.7050\n",
            "Batch [400/1294], Loss: 2.6559\n",
            "Batch [500/1294], Loss: 2.5117\n",
            "Batch [600/1294], Loss: 2.4348\n",
            "Batch [700/1294], Loss: 2.6611\n",
            "Batch [800/1294], Loss: 2.5423\n",
            "Batch [900/1294], Loss: 2.6400\n",
            "Batch [1000/1294], Loss: 2.5653\n",
            "Batch [1100/1294], Loss: 2.4257\n",
            "Batch [1200/1294], Loss: 2.4507\n",
            "Epoch [12/15] Training completed in 1156.60s\n",
            "Average Training Loss: 2.6040\n",
            "Epoch [12/15] - Validation\n",
            "Epoch [12/15], Average Validation Loss: 2.4487\n",
            "Validation loss decreased (2.4801 --> 2.4487). Saving model...\n",
            "EarlyStopping: No significant improvement for 2/3 epochs.\n",
            "\n",
            "Epoch [13/15] - Training\n",
            "Batch [100/1294], Loss: 2.6065\n",
            "Batch [200/1294], Loss: 2.5193\n",
            "Batch [300/1294], Loss: 2.3961\n",
            "Batch [400/1294], Loss: 2.4197\n",
            "Batch [500/1294], Loss: 2.5671\n",
            "Batch [600/1294], Loss: 2.4368\n",
            "Batch [700/1294], Loss: 2.8067\n",
            "Batch [800/1294], Loss: 2.4409\n",
            "Batch [900/1294], Loss: 2.6540\n",
            "Batch [1000/1294], Loss: 2.8351\n",
            "Batch [1100/1294], Loss: 2.7493\n",
            "Batch [1200/1294], Loss: 2.6036\n",
            "Epoch [13/15] Training completed in 1163.73s\n",
            "Average Training Loss: 2.5877\n",
            "Epoch [13/15] - Validation\n",
            "Epoch [13/15], Average Validation Loss: 2.4521\n",
            "Validation loss did not improve significantly from 2.4487.\n",
            "EarlyStopping: No significant improvement for 3/3 epochs.\n",
            "EarlyStopping: Triggering early stop.\n",
            "Early stopping criteria met. Stopping training.\n",
            "\n",
            "--- Resumed Training Finished ---\n",
            "Best Validation Loss achieved: 2.4487\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X438CCfo6uyP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}