{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kush-Singh-26/Image_Caption/blob/main/CaptionColab_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xuYmjPhKra1"
      },
      "source": [
        "# Image Caption Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HVOzgisLC-R"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07hOzTYzu2Xb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "import nltk\n",
        "import json\n",
        "import collections\n",
        "from collections import Counter\n",
        "import random\n",
        "import time\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-lNsV88LH6z"
      },
      "source": [
        "## Importing the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGjHBDsJu7TL"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/coco/images/train2014\n",
        "!mkdir -p /content/coco/images/val2014\n",
        "!mkdir -p /content/coco/annotations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtqN71CEu9lx",
        "outputId": "800ee945-c1ee-4674-f30c-1de6d5298828"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-05-04 17:07:41--  http://images.cocodataset.org/zips/train2014.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 54.231.232.209, 52.216.32.113, 3.5.2.125, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|54.231.232.209|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13510573713 (13G) [application/zip]\n",
            "Saving to: ‘/content/coco/images/train2014.zip’\n",
            "\n",
            "train2014.zip       100%[===================>]  12.58G  42.2MB/s    in 6m 5s   \n",
            "\n",
            "2025-05-04 17:13:46 (35.3 MB/s) - ‘/content/coco/images/train2014.zip’ saved [13510573713/13510573713]\n",
            "\n",
            "--2025-05-04 17:13:46--  http://images.cocodataset.org/zips/val2014.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 3.5.28.192, 52.217.122.161, 16.15.185.235, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|3.5.28.192|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6645013297 (6.2G) [application/zip]\n",
            "Saving to: ‘/content/coco/images/val2014.zip’\n",
            "\n",
            "val2014.zip         100%[===================>]   6.19G  41.6MB/s    in 2m 34s  \n",
            "\n",
            "2025-05-04 17:16:20 (41.2 MB/s) - ‘/content/coco/images/val2014.zip’ saved [6645013297/6645013297]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://images.cocodataset.org/zips/train2014.zip -P /content/coco/images/\n",
        "!wget http://images.cocodataset.org/zips/val2014.zip -P /content/coco/images/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24nq-sl1u_Hb",
        "outputId": "1ce2d949-117b-4b3e-925f-ac14e841a29d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-05-04 17:16:20--  http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 3.5.13.41, 3.5.29.63, 52.216.178.59, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|3.5.13.41|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252872794 (241M) [application/zip]\n",
            "Saving to: ‘/content/coco/annotations/annotations_trainval2014.zip’\n",
            "\n",
            "annotations_trainva 100%[===================>] 241.16M  43.7MB/s    in 6.1s    \n",
            "\n",
            "2025-05-04 17:16:26 (39.4 MB/s) - ‘/content/coco/annotations/annotations_trainval2014.zip’ saved [252872794/252872794]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip -P /content/coco/annotations/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_pnFNrCvBPP"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/coco/images/train2014.zip -d /content/coco/images/\n",
        "!unzip -q /content/coco/images/val2014.zip -d /content/coco/images/\n",
        "!unzip -q /content/coco/annotations/annotations_trainval2014.zip -d /content/coco/annotations/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xr0hvNAuvDL6",
        "outputId": "11536093-4e9b-4493-fe1d-cb1ec79e691a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-05-04 17:20:21--  https://github.com/Delphboy/karpathy-splits/raw/main/dataset_coco.json?download=\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://media.githubusercontent.com/media/Delphboy/karpathy-splits/main/dataset_coco.json?download=true [following]\n",
            "--2025-05-04 17:20:21--  https://media.githubusercontent.com/media/Delphboy/karpathy-splits/main/dataset_coco.json?download=true\n",
            "Resolving media.githubusercontent.com (media.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to media.githubusercontent.com (media.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 408860810 (390M) [application/octet-stream]\n",
            "Saving to: ‘/content/dataset_coco.json’\n",
            "\n",
            "/content/dataset_co 100%[===================>] 389.92M   141MB/s    in 2.8s    \n",
            "\n",
            "2025-05-04 17:20:34 (141 MB/s) - ‘/content/dataset_coco.json’ saved [408860810/408860810]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -c \"https://github.com/Delphboy/karpathy-splits/raw/main/dataset_coco.json?download=\" -O /content/dataset_coco.json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OLl2GrYLNlH"
      },
      "source": [
        "## Splitting the data into train and validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfUIz7XmvFVh",
        "outputId": "16c0d55c-da57-4c57-b1b0-ebde09c8b5ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# Train: 82783 | Val: 5000 | Test: 5000\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "with open(\"/content/dataset_coco.json\", \"r\") as f:\n",
        "    karpathy_data = json.load(f)\n",
        "\n",
        "karpathy_images = karpathy_data['images']\n",
        "\n",
        "# Example: get all train images\n",
        "train_data = [img for img in karpathy_images if img['split'] == 'train']\n",
        "val_data = [img for img in karpathy_images if img['split'] == 'val']\n",
        "test_data = [img for img in karpathy_images if img['split'] == 'test']\n",
        "\n",
        "print(f\"# Train: {len(train_data)} | Val: {len(val_data)} | Test: {len(test_data)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6X1gX-evHP0",
        "outputId": "cb5d48aa-33ab-4786-b6cb-8bdd513d0a74"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import collections\n",
        "from collections import Counter\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72O_1FWoLfEY"
      },
      "source": [
        "## Creating the Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQZzCWd_vKzC"
      },
      "outputs": [],
      "source": [
        "class Vocabulary:\n",
        "    def __init__(self, freq_threshold=5):\n",
        "        self.freq_threshold = freq_threshold\n",
        "        # Initialize with special tokens\n",
        "        self.word2idx = {\"<pad>\": 0, \"<start>\": 1, \"<end>\": 2, \"<unk>\": 3}\n",
        "        self.idx2word = {0: \"<pad>\", 1: \"<start>\", 2: \"<end>\", 3: \"<unk>\"}\n",
        "        self.idx = 4 # Next index to assign\n",
        "\n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        frequencies = Counter()\n",
        "        print(f\"Building vocabulary from {len(sentence_list)} sentences...\")\n",
        "        for i, sentence in enumerate(sentence_list):\n",
        "            tokens = nltk.tokenize.word_tokenize(sentence.lower())\n",
        "            frequencies.update(tokens)\n",
        "            if (i+1) % 100000 == 0:\n",
        "                 print(f\"Processed {i+1}/{len(sentence_list)} sentences\")\n",
        "\n",
        "\n",
        "        original_size = len(frequencies)\n",
        "        filtered_freq = {word: freq for word, freq in frequencies.items() if freq >= self.freq_threshold}\n",
        "\n",
        "        for word in filtered_freq:\n",
        "            if word not in self.word2idx: # Avoid adding duplicates if called multiple times\n",
        "                self.word2idx[word] = self.idx\n",
        "                self.idx2word[self.idx] = word\n",
        "                self.idx += 1\n",
        "        print(f\"Original vocab size: {original_size}, Filtered size (freq>={self.freq_threshold}): {len(self.word2idx)}\")\n",
        "\n",
        "\n",
        "    def numericalize(self, text):\n",
        "        tokens = nltk.tokenize.word_tokenize(text.lower())\n",
        "        return [self.word2idx.get(token, self.word2idx[\"<unk>\"]) for token in tokens]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.idx # Correctly returns the size including special tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIUC4q6BxgIw",
        "outputId": "c29383c6-1463-4dee-f585-c5a19d41da17"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95zKmiYwvdAT",
        "outputId": "e08adc57-50c6-4bed-bf5d-6fc8b4f6433f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building vocabulary from 414113 sentences...\n",
            "Processed 100000/414113 sentences\n",
            "Processed 200000/414113 sentences\n",
            "Processed 300000/414113 sentences\n",
            "Processed 400000/414113 sentences\n",
            "Original vocab size: 24916, Filtered size (freq>=5): 8853\n",
            "Vocabulary Size: 8853\n"
          ]
        }
      ],
      "source": [
        "all_captions = []\n",
        "for img in train_data:\n",
        "    for s in img['sentences']:\n",
        "        all_captions.append(s['raw'])\n",
        "\n",
        "vocab = Vocabulary(freq_threshold=5)\n",
        "vocab.build_vocabulary(all_captions)\n",
        "vocab_size = len(vocab)\n",
        "print(f\"Vocabulary Size: {vocab_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXT5u572vfSd"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"vocab.pkl\", \"wb\") as f:\n",
        "    pickle.dump(vocab, f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7K0tWMULnan"
      },
      "source": [
        "## Encoding using CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtgKy2UQvjD1"
      },
      "outputs": [],
      "source": [
        "class EncoderCNN(nn.Module):\n",
        "  def __init__(self, embed_size, dropout_p=0.5, fine_tune=True):\n",
        "    super(EncoderCNN, self).__init__()\n",
        "    print(f\"Initializing EncoderCNN: embed_size={embed_size}, dropout={dropout_p}, fine_tune={fine_tune}\")\n",
        "    resnet = models.resnet101(weights=models.ResNet101_Weights.IMAGENET1K_V1)\n",
        "\n",
        "    # Freeze all layers initially\n",
        "    for param in resnet.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "    # Fine-tuning: Unfreeze later layers if fine_tune is True\n",
        "    if fine_tune:\n",
        "      print(\"Fine-tuning ResNet: Unfreezing layer4 parameters.\")\n",
        "      for param in resnet.layer4.parameters(): # Unfreeze layer 4\n",
        "          param.requires_grad = True\n",
        "\n",
        "    # Remove the final classification layer\n",
        "    self.resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
        "\n",
        "    # Add trainable layers\n",
        "    self.fc = nn.Linear(resnet.fc.in_features, embed_size)\n",
        "    self.bn = nn.BatchNorm1d(embed_size, momentum=0.01) # BatchNorm after FC\n",
        "    self.dropout = nn.Dropout(dropout_p) # Dropout layer\n",
        "\n",
        "    # Initialize weights for the new layers\n",
        "    self.fc.weight.data.normal_(0.0, 0.02)\n",
        "    self.fc.bias.data.fill_(0)\n",
        "\n",
        "  def forward(self, images):\n",
        "    with torch.no_grad() if not self.training else torch.enable_grad(): # Only track gradients during training for ResNet parts if fine-tuning\n",
        "        features = self.resnet(images) # [B, C, 1, 1]\n",
        "\n",
        "    features = features.squeeze(3).squeeze(2) # [B, C]\n",
        "    features = self.fc(features)              # [B, E]\n",
        "    features = self.bn(features)              # [B, E] - Apply BN before dropout\n",
        "    features = self.dropout(features)         # [B, E] - Apply dropout\n",
        "    return features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0RaeDMURJK9"
      },
      "source": [
        "## Decoding the feature map of image using LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1STLQa5v5jP"
      },
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1, dropout_p=0.5):\n",
        "        super().__init__()\n",
        "        print(f\"Initializing DecoderRNN: embed_size={embed_size}, hidden_size={hidden_size}, vocab_size={vocab_size}, num_layers={num_layers}, dropout={dropout_p}\")\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.embed_dropout = nn.Dropout(dropout_p) # Dropout after embedding\n",
        "        # Apply LSTM dropout between layers only if num_layers > 1\n",
        "        lstm_dropout = dropout_p if num_layers > 1 else 0\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=lstm_dropout)\n",
        "        self.dropout = nn.Dropout(dropout_p) # Dropout before final linear layer\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        # Layers to initialize LSTM state from image features\n",
        "        self.init_h = nn.Linear(embed_size, hidden_size)\n",
        "        self.init_c = nn.Linear(embed_size, hidden_size)\n",
        "\n",
        "        # Initialize weights\n",
        "        self.embed.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.linear.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.linear.bias.data.fill_(0)\n",
        "        self.init_h.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.init_h.bias.data.fill_(0)\n",
        "        self.init_c.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.init_c.bias.data.fill_(0)\n",
        "\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        # features: [B, E], captions: [B, T] (T = sequence length)\n",
        "\n",
        "        # Prepare initial LSTM state from image features\n",
        "        # Need shape [num_layers, B, H]\n",
        "        h0 = self.init_h(features).unsqueeze(0) # [1, B, H]\n",
        "        c0 = self.init_c(features).unsqueeze(0) # [1, B, H]\n",
        "        # If num_layers > 1, repeat the initial state for each layer\n",
        "        if self.lstm.num_layers > 1:\n",
        "             h0 = h0.repeat(self.lstm.num_layers, 1, 1)\n",
        "             c0 = c0.repeat(self.lstm.num_layers, 1, 1)\n",
        "\n",
        "        # Embed captions and apply dropout\n",
        "        embeddings = self.embed(captions)    # [B, T, E]\n",
        "        embeddings = self.embed_dropout(embeddings) # Apply dropout\n",
        "\n",
        "        # Pass through LSTM\n",
        "        # embeddings shape needs to be [B, T, E] for batch_first=True\n",
        "        hiddens, _ = self.lstm(embeddings, (h0, c0))  # [B, T, H]\n",
        "\n",
        "        # Apply dropout before the final linear layer\n",
        "        hiddens = self.dropout(hiddens) # Apply dropout\n",
        "\n",
        "        # Generate outputs (logits over vocabulary)\n",
        "        outputs = self.linear(hiddens)  # [B, T, Vocab]\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3rqds5EU8z_"
      },
      "source": [
        "## Prepare the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_uerJvcv9Bz"
      },
      "outputs": [],
      "source": [
        "class CocoDataset(Dataset):\n",
        "    def __init__(self, data, img_root, vocab, transform=None):\n",
        "        self.data = data\n",
        "        self.img_root = img_root\n",
        "        self.vocab = vocab\n",
        "        self.transform = transform\n",
        "        print(f\"Initialized CocoDataset with {len(self.data)} items. Root: {self.img_root}\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        entry = self.data[idx]\n",
        "\n",
        "        # --- Randomly select one caption ---\n",
        "        caption_entry = random.choice(entry['sentences'])\n",
        "        caption = caption_entry['raw']\n",
        "\n",
        "        # Numericalize caption\n",
        "        tokens = [self.vocab.word2idx[\"<start>\"]] + \\\n",
        "                 self.vocab.numericalize(caption) + \\\n",
        "                 [self.vocab.word2idx[\"<end>\"]]\n",
        "\n",
        "        # Load image\n",
        "        split_folder = entry.get('filepath', '')\n",
        "\n",
        "        if split_folder in self.img_root:\n",
        "             img_path = os.path.join(self.img_root, entry['filename'])\n",
        "        else:\n",
        "             img_path = os.path.join(self.img_root, split_folder, entry['filename'])\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "        except FileNotFoundError:\n",
        "             print(f\"Warning: Image not found at {img_path}. Skipping.\")\n",
        "\n",
        "             return self.__getitem__(0)\n",
        "\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, torch.Tensor(tokens).long() # Ensure tokens are LongTensor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqDqcucaU_yp"
      },
      "source": [
        "## Transformations to be applied on the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGJ7Os3qwM_g"
      },
      "outputs": [],
      "source": [
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "\n",
        "# Training Transform (with augmentation)\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)), # Random crop and resize\n",
        "    transforms.RandomHorizontalFlip(),      # Random horizontal flip\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), # Color augmentation\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "# Validation/Test Transform\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSXWKWhXVXf3"
      },
      "source": [
        "## Make all captions in a batch equal in length by padding them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7LnxrIAwR-f"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    # Separate images and captions\n",
        "    images, captions = zip(*batch)\n",
        "\n",
        "    # Stack images\n",
        "    images = torch.stack(images, 0)\n",
        "\n",
        "    # Pad captions to the max length in the batch\n",
        "    captions = pad_sequence(captions, batch_first=True, padding_value=vocab.word2idx[\"<pad>\"])\n",
        "\n",
        "    return images, captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzKBMi4iwTre"
      },
      "outputs": [],
      "source": [
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "num_layers = 1         # Number of LSTM layers\n",
        "dropout_prob = 0.5     # Dropout probability\n",
        "batch_size = 64       # Increased batch size \n",
        "num_epochs = 15       # Reduced epochs as it was overfitting quickly\n",
        "learning_rate = 3e-4   # Adjusted learning rate\n",
        "fine_tune_lr = 1e-5    # Separate learning rate for fine-tuned layers\n",
        "weight_decay = 1e-5    # Weight decay (L2 regularization)\n",
        "fine_tune_encoder = True # Set to True to for fine-tuning ResNet layer4\n",
        "patience_early_stop = 3\n",
        "patience_scheduler = 1\n",
        "delta_early_stop = 0.005 # Require a minimum improvement to reset counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZ9GJ7snwWPM",
        "outputId": "6c0087e0-e69b-4419-b9a5-696f55f5dd56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Initialized CocoDataset with 82783 items. Root: /content/coco/images/\n",
            "Initialized CocoDataset with 5000 items. Root: /content/coco/images/\n",
            "Using 2 workers for DataLoaders.\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "train_img_root = '/content/coco/images/'\n",
        "val_img_root = '/content/coco/images/'\n",
        "\n",
        "train_dataset = CocoDataset(train_data, train_img_root, vocab, train_transform)\n",
        "val_dataset = CocoDataset(val_data, val_img_root, vocab, val_transform)\n",
        "\n",
        "num_workers = 2 if device.type == 'cuda' else 0\n",
        "print(f\"Using {num_workers} workers for DataLoaders.\")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=num_workers, pin_memory=True if device.type == 'cuda' else False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=num_workers, pin_memory=True if device.type == 'cuda' else False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhHnJr7FwtTK",
        "outputId": "9258485e-ba96-4d53-9537-430d410e7bdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Initializing Models, Optimizer, etc. ---\n",
            "Initializing EncoderCNN: embed_size=256, dropout=0.5, fine_tune=True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n",
            "100%|██████████| 171M/171M [00:01<00:00, 168MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fine-tuning ResNet: Unfreezing layer4 parameters.\n",
            "Initializing DecoderRNN: embed_size=256, hidden_size=512, vocab_size=8853, num_layers=1, dropout=0.5\n",
            "Optimizing 30 parameter tensors from ResNet layer4 with LR 1e-05\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "print(\"--- Initializing Models, Optimizer, etc. ---\")\n",
        "encoder = EncoderCNN(embed_size, dropout_p=dropout_prob, fine_tune=fine_tune_encoder).to(device)\n",
        "decoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers, dropout_p=dropout_prob).to(device)\n",
        "\n",
        "decoder_params = list(decoder.parameters())\n",
        "encoder_fc_params = list(encoder.fc.parameters()) + list(encoder.bn.parameters())\n",
        "encoder_finetune_params = []\n",
        "if fine_tune_encoder:\n",
        "    layer4_index = 7\n",
        "    encoder_finetune_params = list(encoder.resnet[layer4_index].parameters())\n",
        "    print(f\"Optimizing {len(encoder_finetune_params)} parameter tensors from ResNet layer4 with LR {fine_tune_lr}\")\n",
        "\n",
        "\n",
        "params_to_optimize = [\n",
        "    {'params': decoder_params},\n",
        "    {'params': encoder_fc_params}\n",
        "]\n",
        "\n",
        "if encoder_finetune_params:\n",
        "    params_to_optimize.append({'params': encoder_finetune_params, 'lr': fine_tune_lr})\n",
        "\n",
        "optimizer = optim.Adam(params_to_optimize, lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "# Learning Rate Scheduler\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=patience_scheduler, verbose=True)\n",
        "\n",
        "# Loss Function (ignore padding)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=vocab.word2idx[\"<pad>\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80a_G3zTWBXA"
      },
      "source": [
        "## Early Stopping mechanism to prevent overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Mxt3Clew3Yo",
        "outputId": "2a4e8647-d5ec-4fa0-acb8-8e354c1f0a9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialized EarlyStopping: patience=3, delta=0.005\n"
          ]
        }
      ],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3, delta=0.0):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "        self.delta = delta # Minimum change to qualify as an improvement\n",
        "        print(f\"Initialized EarlyStopping: patience={patience}, delta={delta}\")\n",
        "\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "            print(f\"EarlyStopping: Initial best loss set to {val_loss:.4f}\")\n",
        "        # Check if val_loss has improved significantly\n",
        "        elif val_loss < self.best_loss - self.delta:\n",
        "             print(f\"EarlyStopping: Validation loss improved ({self.best_loss:.4f} --> {val_loss:.4f}). Resetting counter.\")\n",
        "             self.best_loss = val_loss\n",
        "             self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            print(f\"EarlyStopping: No significant improvement for {self.counter}/{self.patience} epochs.\")\n",
        "            if self.counter >= self.patience:\n",
        "                print(\"EarlyStopping: Triggering early stop.\")\n",
        "                self.early_stop = True\n",
        "\n",
        "early_stopper = EarlyStopping(patience=patience_early_stop, delta=delta_early_stop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzrOEB0Iw8KK",
        "outputId": "848b6132-2b6b-483b-8ff1-b3dc29f943a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Model Summary ---\n",
            "Trainable Parameters in Encoder: 15,489,792\n",
            "Trainable Parameters in Decoder: 8,648,085\n",
            "Total Trainable Parameters: 24,137,877\n",
            "---------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "total_params_encoder = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
        "total_params_decoder = sum(p.numel() for p in decoder.parameters() if p.requires_grad)\n",
        "print(f\"\\n--- Model Summary ---\")\n",
        "print(f\"Trainable Parameters in Encoder: {total_params_encoder:,}\")\n",
        "print(f\"Trainable Parameters in Decoder: {total_params_decoder:,}\")\n",
        "print(f\"Total Trainable Parameters: {total_params_encoder + total_params_decoder:,}\")\n",
        "print(\"---------------------\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "I60IIFFMw__X",
        "outputId": "47644e65-061d-428d-9580-15886bf5ee63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Starting Training ---\n",
            "\n",
            "Epoch [1/15] - Training\n",
            "Batch [100/1294], Loss: 5.0666\n",
            "Batch [200/1294], Loss: 4.6535\n",
            "Batch [300/1294], Loss: 4.5354\n",
            "Batch [400/1294], Loss: 4.0987\n",
            "Batch [500/1294], Loss: 4.1721\n",
            "Batch [600/1294], Loss: 4.0385\n",
            "Batch [700/1294], Loss: 3.6736\n",
            "Batch [800/1294], Loss: 3.7483\n",
            "Batch [900/1294], Loss: 3.8578\n",
            "Batch [1000/1294], Loss: 3.7661\n",
            "Batch [1100/1294], Loss: 3.7105\n",
            "Batch [1200/1294], Loss: 3.3711\n",
            "Epoch [1/15] Training completed in 1257.06s\n",
            "Average Training Loss: 4.1539\n",
            "Epoch [1/15] - Validation\n",
            "Epoch [1/15], Average Validation Loss: 3.2645\n",
            "Validation loss decreased (inf --> 3.2645). Saving model...\n",
            "EarlyStopping: Initial best loss set to 3.2645\n",
            "\n",
            "Epoch [2/15] - Training\n",
            "Batch [100/1294], Loss: 3.3486\n",
            "Batch [200/1294], Loss: 3.2202\n",
            "Batch [300/1294], Loss: 3.3123\n",
            "Batch [400/1294], Loss: 3.3849\n",
            "Batch [500/1294], Loss: 3.3364\n",
            "Batch [600/1294], Loss: 3.3308\n",
            "Batch [700/1294], Loss: 3.6197\n",
            "Batch [800/1294], Loss: 3.0635\n",
            "Batch [900/1294], Loss: 3.1723\n",
            "Batch [1000/1294], Loss: 3.1952\n",
            "Batch [1100/1294], Loss: 3.1806\n",
            "Batch [1200/1294], Loss: 3.2827\n",
            "Epoch [2/15] Training completed in 1258.77s\n",
            "Average Training Loss: 3.2466\n",
            "Epoch [2/15] - Validation\n",
            "Epoch [2/15], Average Validation Loss: 2.9503\n",
            "Validation loss decreased (3.2645 --> 2.9503). Saving model...\n",
            "EarlyStopping: Validation loss improved (3.2645 --> 2.9503). Resetting counter.\n",
            "\n",
            "Epoch [3/15] - Training\n",
            "Batch [100/1294], Loss: 3.1719\n",
            "Batch [200/1294], Loss: 3.0589\n",
            "Batch [300/1294], Loss: 2.9462\n",
            "Batch [400/1294], Loss: 2.9945\n",
            "Batch [500/1294], Loss: 3.1215\n",
            "Batch [600/1294], Loss: 3.1952\n",
            "Batch [700/1294], Loss: 3.0779\n",
            "Batch [800/1294], Loss: 3.1695\n",
            "Batch [900/1294], Loss: 3.1003\n",
            "Batch [1000/1294], Loss: 2.8816\n",
            "Batch [1100/1294], Loss: 2.8644\n",
            "Batch [1200/1294], Loss: 3.0632\n",
            "Epoch [3/15] Training completed in 1264.41s\n",
            "Average Training Loss: 3.0404\n",
            "Epoch [3/15] - Validation\n",
            "Epoch [3/15], Average Validation Loss: 2.7930\n",
            "Validation loss decreased (2.9503 --> 2.7930). Saving model...\n",
            "EarlyStopping: Validation loss improved (2.9503 --> 2.7930). Resetting counter.\n",
            "\n",
            "Epoch [4/15] - Training\n",
            "Batch [100/1294], Loss: 2.6697\n",
            "Batch [200/1294], Loss: 2.7722\n",
            "Batch [300/1294], Loss: 3.0046\n",
            "Batch [400/1294], Loss: 3.1165\n",
            "Batch [500/1294], Loss: 3.1506\n",
            "Batch [600/1294], Loss: 3.0090\n",
            "Batch [700/1294], Loss: 3.0058\n",
            "Batch [800/1294], Loss: 3.0854\n",
            "Batch [900/1294], Loss: 2.9973\n",
            "Batch [1000/1294], Loss: 2.9663\n",
            "Batch [1100/1294], Loss: 2.7401\n",
            "Batch [1200/1294], Loss: 2.8667\n",
            "Epoch [4/15] Training completed in 1277.25s\n",
            "Average Training Loss: 2.9184\n",
            "Epoch [4/15] - Validation\n",
            "Epoch [4/15], Average Validation Loss: 2.7084\n",
            "Validation loss decreased (2.7930 --> 2.7084). Saving model...\n",
            "EarlyStopping: Validation loss improved (2.7930 --> 2.7084). Resetting counter.\n",
            "\n",
            "Epoch [5/15] - Training\n",
            "Batch [100/1294], Loss: 2.8767\n",
            "Batch [200/1294], Loss: 2.7879\n",
            "Batch [300/1294], Loss: 2.8082\n",
            "Batch [400/1294], Loss: 2.8480\n",
            "Batch [500/1294], Loss: 2.7559\n",
            "Batch [600/1294], Loss: 2.9059\n",
            "Batch [700/1294], Loss: 2.9011\n",
            "Batch [800/1294], Loss: 2.9529\n",
            "Batch [900/1294], Loss: 2.8949\n",
            "Batch [1000/1294], Loss: 2.7163\n",
            "Batch [1100/1294], Loss: 2.8211\n",
            "Batch [1200/1294], Loss: 2.6987\n",
            "Epoch [5/15] Training completed in 1274.61s\n",
            "Average Training Loss: 2.8451\n",
            "Epoch [5/15] - Validation\n",
            "Epoch [5/15], Average Validation Loss: 2.6369\n",
            "Validation loss decreased (2.7084 --> 2.6369). Saving model...\n",
            "EarlyStopping: Validation loss improved (2.7084 --> 2.6369). Resetting counter.\n",
            "\n",
            "Epoch [6/15] - Training\n",
            "Batch [100/1294], Loss: 2.7026\n",
            "Batch [200/1294], Loss: 2.7574\n",
            "Batch [300/1294], Loss: 2.7721\n",
            "Batch [400/1294], Loss: 2.6477\n",
            "Batch [500/1294], Loss: 2.7484\n",
            "Batch [600/1294], Loss: 2.7042\n",
            "Batch [700/1294], Loss: 2.7531\n",
            "Batch [800/1294], Loss: 2.9168\n",
            "Batch [900/1294], Loss: 2.5309\n",
            "Batch [1000/1294], Loss: 2.7749\n",
            "Batch [1100/1294], Loss: 2.7656\n",
            "Batch [1200/1294], Loss: 2.5915\n",
            "Epoch [6/15] Training completed in 1269.71s\n",
            "Average Training Loss: 2.7842\n",
            "Epoch [6/15] - Validation\n",
            "Epoch [6/15], Average Validation Loss: 2.6360\n",
            "Validation loss did not improve significantly from 2.6369.\n",
            "EarlyStopping: No significant improvement for 1/3 epochs.\n",
            "\n",
            "Epoch [7/15] - Training\n",
            "Batch [100/1294], Loss: 2.8435\n",
            "Batch [200/1294], Loss: 2.8320\n",
            "Batch [300/1294], Loss: 2.7611\n",
            "Batch [400/1294], Loss: 2.9558\n",
            "Batch [500/1294], Loss: 2.8353\n",
            "Batch [600/1294], Loss: 2.7889\n",
            "Batch [700/1294], Loss: 2.8865\n",
            "Batch [800/1294], Loss: 2.7262\n",
            "Batch [900/1294], Loss: 2.7498\n",
            "Batch [1000/1294], Loss: 2.5306\n",
            "Batch [1100/1294], Loss: 2.5986\n",
            "Batch [1200/1294], Loss: 2.9247\n",
            "Epoch [7/15] Training completed in 1269.31s\n",
            "Average Training Loss: 2.7413\n",
            "Epoch [7/15] - Validation\n",
            "Epoch [7/15], Average Validation Loss: 2.5658\n",
            "Validation loss decreased (2.6369 --> 2.5658). Saving model...\n",
            "EarlyStopping: Validation loss improved (2.6369 --> 2.5658). Resetting counter.\n",
            "\n",
            "Epoch [8/15] - Training\n",
            "Batch [100/1294], Loss: 2.6638\n",
            "Batch [200/1294], Loss: 2.9145\n",
            "Batch [300/1294], Loss: 2.5617\n",
            "Batch [400/1294], Loss: 2.7330\n",
            "Batch [500/1294], Loss: 2.9906\n",
            "Batch [600/1294], Loss: 2.8912\n",
            "Batch [700/1294], Loss: 2.6356\n",
            "Batch [800/1294], Loss: 2.8117\n",
            "Batch [900/1294], Loss: 2.4630\n",
            "Batch [1000/1294], Loss: 2.8515\n",
            "Batch [1100/1294], Loss: 2.7842\n",
            "Batch [1200/1294], Loss: 2.5701\n",
            "Epoch [8/15] Training completed in 1276.73s\n",
            "Average Training Loss: 2.7025\n",
            "Epoch [8/15] - Validation\n",
            "Epoch [8/15], Average Validation Loss: 2.5587\n",
            "Validation loss decreased (2.5658 --> 2.5587). Saving model...\n",
            "EarlyStopping: Validation loss improved (2.5658 --> 2.5587). Resetting counter.\n",
            "\n",
            "Epoch [9/15] - Training\n",
            "Batch [100/1294], Loss: 2.6862\n",
            "Batch [200/1294], Loss: 2.5054\n",
            "Batch [300/1294], Loss: 2.7092\n",
            "Batch [400/1294], Loss: 2.8623\n",
            "Batch [500/1294], Loss: 2.8302\n",
            "Batch [600/1294], Loss: 2.7284\n",
            "Batch [700/1294], Loss: 2.6537\n",
            "Batch [800/1294], Loss: 2.5088\n",
            "Batch [900/1294], Loss: 2.9458\n",
            "Batch [1000/1294], Loss: 2.6836\n",
            "Batch [1100/1294], Loss: 2.5510\n",
            "Batch [1200/1294], Loss: 2.7055\n",
            "Epoch [9/15] Training completed in 1272.30s\n",
            "Average Training Loss: 2.6722\n",
            "Epoch [9/15] - Validation\n",
            "Epoch [9/15], Average Validation Loss: 2.5286\n",
            "Validation loss decreased (2.5587 --> 2.5286). Saving model...\n",
            "EarlyStopping: Validation loss improved (2.5587 --> 2.5286). Resetting counter.\n",
            "\n",
            "Epoch [10/15] - Training\n",
            "Batch [100/1294], Loss: 2.5900\n",
            "Batch [200/1294], Loss: 2.5835\n",
            "Batch [300/1294], Loss: 2.8218\n",
            "Batch [400/1294], Loss: 2.7197\n",
            "Batch [500/1294], Loss: 2.5483\n",
            "Batch [600/1294], Loss: 2.3585\n",
            "Batch [700/1294], Loss: 2.5230\n",
            "Batch [800/1294], Loss: 2.4840\n",
            "Batch [900/1294], Loss: 2.7847\n",
            "Batch [1000/1294], Loss: 2.7313\n",
            "Batch [1100/1294], Loss: 2.7844\n",
            "Batch [1200/1294], Loss: 2.7854\n",
            "Epoch [10/15] Training completed in 1272.80s\n",
            "Average Training Loss: 2.6457\n",
            "Epoch [10/15] - Validation\n",
            "Epoch [10/15], Average Validation Loss: 2.4801\n",
            "Validation loss decreased (2.5286 --> 2.4801). Saving model...\n",
            "EarlyStopping: Validation loss improved (2.5286 --> 2.4801). Resetting counter.\n",
            "\n",
            "Epoch [11/15] - Training\n",
            "Batch [100/1294], Loss: 2.6197\n",
            "Batch [200/1294], Loss: 2.5760\n",
            "Batch [300/1294], Loss: 2.7678\n",
            "Batch [400/1294], Loss: 2.6696\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-47fa989378ed>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nEpoch [{epoch+1}/{num_epochs}] - Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1459\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1410\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1412\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "print(\"--- Starting Training ---\")\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    epoch_train_loss = 0.0\n",
        "\n",
        "    # --- Training Phase ---\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    print(f\"\\nEpoch [{epoch+1}/{num_epochs}] - Training\")\n",
        "    for i, (images, captions) in enumerate(train_loader):\n",
        "        images, captions = images.to(device), captions.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        features = encoder(images)\n",
        "        # Teacher forcing: Feed target captions (shifted) to decoder\n",
        "        # Input: <start>, w1, w2, ... wn\n",
        "        # Target: w1, w2, ... wn, <end>\n",
        "        outputs = decoder(features, captions[:, :-1]) # Exclude <end> token for input\n",
        "\n",
        "        # Calculate loss\n",
        "        # Target is captions shifted left, excluding <start> token\n",
        "        loss = criterion(outputs.reshape(-1, vocab_size), captions[:, 1:].reshape(-1))\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_train_loss += loss.item()\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"Batch [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_train_loss = epoch_train_loss / len(train_loader)\n",
        "    epoch_time = time.time() - start_time\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Training completed in {epoch_time:.2f}s\")\n",
        "    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Validation Phase\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    epoch_val_loss = 0.0\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Validation\")\n",
        "    with torch.no_grad():\n",
        "        for val_images, val_captions in val_loader:\n",
        "            val_images, val_captions = val_images.to(device), val_captions.to(device)\n",
        "\n",
        "            val_features = encoder(val_images)\n",
        "            val_outputs = decoder(val_features, val_captions[:, :-1]) # Exclude <end> token\n",
        "\n",
        "            val_loss = criterion(val_outputs.reshape(-1, vocab_size), val_captions[:, 1:].reshape(-1))\n",
        "            epoch_val_loss += val_loss.item()\n",
        "\n",
        "    avg_val_loss = epoch_val_loss / len(val_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    if avg_val_loss < best_val_loss - delta_early_stop: # Use delta here too for saving\n",
        "        print(f\"Validation loss decreased ({best_val_loss:.4f} --> {avg_val_loss:.4f}). Saving model...\")\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'encoder_state_dict': encoder.state_dict(),\n",
        "            'decoder_state_dict': decoder.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'vocab': vocab, # Save vocab for inference later\n",
        "            'val_loss': best_val_loss,\n",
        "            'embed_size': embed_size,\n",
        "            'hidden_size': hidden_size,\n",
        "            'num_layers': num_layers,\n",
        "            'dropout_prob': dropout_prob,\n",
        "            'fine_tune_encoder': fine_tune_encoder\n",
        "        }, 'best_model_improved.pth')\n",
        "    else:\n",
        "         print(f\"Validation loss did not improve significantly from {best_val_loss:.4f}.\")\n",
        "\n",
        "    # Learning Rate Scheduling\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "    # Early Stopping Check\n",
        "    early_stopper(avg_val_loss)\n",
        "    if early_stopper.early_stop:\n",
        "        print(\"Early stopping criteria met. Stopping training.\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6rsQcGwW1mQ"
      },
      "source": [
        "# A bit of training was left as the colab session was over. So it is continued in the next colab notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJklRJYWxH0_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
